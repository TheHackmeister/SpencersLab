
bitwardenIds:
  keycloak: OVERRIDE_VIA_CUSTOM_VALUES
  keycloak-pg: OVERRIDE_VIA_CUSTOM_VALUES
  smtp: OVERRIDE_VIA_CUSTOM_VALUES
  gitea: OVERRIDE_VIA_CUSTOM_VALUES
  gitea-admin: OVERRIDE_VIA_CUSTOM_VALUES

domain: OVERRIDE_VIA_APPSET
clusterName: OVERRIDE_VIA_APPSET

ingress:
  subdomains:
    login:
      #serviceName: keycloakx
      service: infra-keycloakx-http
      port: 80

    seaweedfs-filer:
      serviceName: seaweedfs-filer.infra
      service: seadweedfs-filer
      port: 8888

    seaweedfs-volume:
      serviceName: seaweedfs-volume.infra
      service: seaweedfs-volume
      port: 8080

    seaweedfs-master:
      serviceName: seaweedfs-master.infra
      service: seaweedfs-master
      port: 9333

    s3:
      #serviceName: seaweedfs-s3.infra
      service: seaweedfs-s3
      port: 8333

    #git:
    #  serviceName: forgejo
    #  service: infra-forgeo
    #  port: ????

    cluster:
      clusterBase: true
      service: base-argocd-server
      port: 80
    traefik:
      clusterBase: true
      namespace: kube-system
      service: traefik
      port: 80

prometheus:
  serviceMonitorSelectorNilUsesHelmValues: false

389ds:
  bitwardenIds:
    389ds: f7b628ec-2544-4c28-a476-b1a501487d99

  app-template:
    controllers:
      389ds:
        containers:
          main:
            image:
              repository: 389ds/dirsrv
              tag: 3.1
            env:
              DS_SUFFIX_NAME: dc=spencerslab,dc=com

    configMaps:
      ldifs:
        # TO initalize these, need to manually run something like:
        #  dsconf slapd-localhost backend create --suffix "dc=spencerslab,dc=com" --be-name userRoot
        #  ldapadd -x -D "cn=Directory Manager" -W -f /tmp/add_suffix.ldif -H ldap://localhost:3389
        data: 
          00-root.ldif: |-
            # Root creation
            dn: dc=spencerslab,dc=com
            objectClass: top
            objectClass: domain
            dc: SpencersLab

          01-people-ou.ldif: |
            dn: ou=People,dc=spencerslab,dc=com
            objectClass: organizationalUnit
            ou: People
          02-groups-ou.ldif: |
            dn: ou=Groups,dc=spencerslab,dc=com
            objectClass: organizationalUnit
            ou: Groups
  
          # TODO: I think there is something wrong with these. Running
          # dsidm localhost group list
          # after creating these doesn't return anything.
          11-guest-group.ldif: |-
            dn: cn=Guests,ou=Groups,dc=spencerslab,dc=com
            changetype: add
            objectClass: top
            objectClass: posixGroup
            objectClass: groupOfUniqueNames
            gidNumber: 12345
            cn: Guests
          12-user-group.ldif: |-
            dn: cn=Users,ou=Groups,dc=spencerslab,dc=com
            changetype: add
            objectClass: top
            objectClass: posixGroup
            objectClass: groupOfUniqueNames
            gidNumber: 12346
            cn: Users
          13-power-user-group.ldif: |-
            dn: cn=PowerUsers,ou=Groups,dc=spencerslab,dc=com
            changetype: add
            objectClass: top
            objectClass: posixGroup
            objectClass: groupOfUniqueNames
            gidNumber: 12347
            cn: PowerUsers
          14-admin-group.ldif: |-
            dn: cn=Admins,ou=Groups,dc=spencerslab,dc=com
            changetype: add
            objectClass: top
            objectClass: posixGroup
            objectClass: groupOfUniqueNames
            gidNumber: 12348
            cn: Admins

    persistence:
      ldifs:
        type: configMap
        name: '{{ .Release.Name }}-389ds-ldifs'
        globalMounts:
          - path: /data/ldif/

keycloakx:
    image:  
      repository: quay.io/keycloak/keycloak
      tag: 26.1.3

    command:
      - "/opt/keycloak/bin/kc.sh"
      - "--verbose"
      - "start"

    database:
      vendor: "postgres"
      hostname: "pg-keycloak-rw"
      port: "5432"
      database: "keycloak"
      user: "keycloak"
      existingSecret: "pg-keycloak-secret"
      existingSecretKey: "password"
    dbchecker:
      enabled: true

    http:
      relativePath: /
      internalPort: metrics

    proxy:
      enabled: true
      mode: xforwarded

    extraEnv: |-
      - name: TZ
        value: "America/Denver"
      - name: JAVA_OPTS_APPEND
        value: >-
          -Djava.awt.headless=true
          -Djgroups.dns.query=keycloak-headless
          -XX:+UseContainerSupport
      #- name: PROXY_ADDRESS_FORWARDING
      #  value: "true"
      - name: KC_HTTP_PORT
        value: "8080"
      - name: KC_HOSTNAME
        value: https://login.spencerslab.com/
      - name: KC_HOSTNAME_STRICT
        value: "true"
      - name: KC_HOSTNAME_STRICT_HTTPS
        value: "true"
      - name: KC_LOG_LEVEL
        value: "org.keycloak.events:DEBUG,org.infinispan:INFO,org.jgroups:INFO"
      - name: KEYCLOAK_ADMIN
        valueFrom:
          secretKeyRef:
            name: keycloak-secret
            key: KEYCLOAK_ADMIN
      - name: KEYCLOAK_ADMIN_PASSWORD
        valueFrom:
          secretKeyRef:
            name: keycloak-secret
            key: KEYCLOAK_ADMIN_PASSWORD
      
    extraVolumes: |-
      - name: keycloak-data
        persistentVolumeClaim:
          claimName: keycloak
      - name: keycloak-providers
        persistentVolumeClaim:
          claimName: keycloak-providers
      - name: proxy-allowlist
        persistentVolumeClaim:
          claimName: proxy-allowlist
    # - name: customreg
    #   secret:
    #     secretName: {{ include "keycloak.fullname" . }}-customreg
    # - name: realm
    #   secret:
    #     secretName: {{ include "keycloak.fullname" . }}-realm
    # - name: plugin
    #   emptyDir: {}
    # - name: truststore
    #   secret:
    #     secretName: {{ include "keycloak.fullname" . }}-truststore
    # - name: custom-theme
    #   emptyDir: {}
    extraVolumeMounts: |-
      - name: keycloak-data
        mountPath: /opt/keycloak/data/
      - name: keycloak-providers
        mountPath: /opt/keycloak/providers/
      # Mounting at /data/ because that's where traefik reads it from. 
      # In this pod it might work elsewhere. Would have to update the SPI.
      - name: proxy-allowlist
        mountPath: /data/

    metrics:
      enabled: true

    serviceMonitor:
      enabled: true
      labels:
        release: monitoring-agent
      port: metrics

    extraServiceMonitor:
      enabled: true  
      labels:
        release: monitoring-agent
      port: http
      path: /realms/master/metrics

generic-device-plugin:
  app-template:
    configMaps:
      config:
        enabled: true
        data:
          devices.yaml: |-
            devices:
              - name: hdd
                groups:
                  - paths:
                      - path: /var/mnt/drive1
                        mountPath: /drive
                      - path: /var/mnt/drive2
                        mountPath: /drive
                      - path: /var/mnt/drive3
                        mountPath: /drive
                      - path: /var/mnt/drive4
                        mountPath: /drive
                      - path: /var/mnt/drive5
                        mountPath: /drive

seaweedfs-csi-driver:
  #seaweedfsFiler: "seaweedfs-filer.{{ $.Values.clusterName }}.{{ $.Values.domain }}"
  seaweedfsFiler: "seaweedfs-filer-0.seaweedfs-filer.default:8888"

seaweedfs:
  global:
    createClusterRole: true
    monitoring:
      enabled: true
    # if enabled will use global.replicationPlacment and override master & filer defaultReplicaPlacement config
    enableReplication: true
    #  replication type is XYZ:
    # X number of replica in other data centers
    # Y number of replica in other racks in the same data center
    # Z number of replica in other servers in the same rack
    replicationPlacement: "001"

    extraEnvironmentVars:
      WEED_CLUSTER_DEFAULT: "sw"
      WEED_CLUSTER_SW_MASTER: "seaweedfs-master.default:9333"
      WEED_CLUSTER_SW_FILER: "seaweedfs-filer-client.default:8888"

  master:
    enabled: true
    #port: 9333
    #grpcPort: 19333
    metricsPort: 9327
    volumeSizeLimitMB: 30000
    # This default is currently overridden globally.
    # defaultReplication: "000"

    #config: |-
    #  # Enter any extra configuration for master.toml here.
    #  # It may be be a multi-line string.

    # You may use ANY storage-class, example with local-path-provisioner
    # Annotations are optional.
    #  data:
    #    type: "persistentVolumeClaim"
    #    size: "24Ti"
    #    storageClass: "local-path-provisioner"
    #    annotations:
    #      "key": "value"
    #
    # You may also spacify an existing claim:
    #  data:
    #    type: "existingClaim"
    #    claimName: "my-pvc"
    #
    # You can also use emptyDir storage:
    #  data:
    #    type: "emptyDir"
    data:
      type: "persistentVolumeClaim"
      size: 10Gi
    logs:
      type: "existingClaim"
      claimName: "data-default-seaweedfs-master-0"

    # Resource requests, limits, etc. for the master cluster placement. This
    # should map directly to the value of the resources field for a PodSpec,
    # formatted as a multi-line string. By default no direct resource request
    # is made.
    resources: {}

    # Affinity Settings
    # Commenting out or setting as empty the affinity variable, will allow
    # deployment to single node services such as Minikube
    affinity: {}
      #|
      #podAntiAffinity:
      #  requiredDuringSchedulingIgnoredDuringExecution:
      #    - labelSelector:
      #        matchLabels:
      #          app.kubernetes.io/name: {{ template "seaweedfs.name" . }}
      #          app.kubernetes.io/instance: {{ .Release.Name }}
      #          app.kubernetes.io/component: master
      #      topologyKey: kubernetes.io/hostname

    # Configure security context for Pod
    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    # Example:
    # podSecurityContext:
    #   enabled: true
    #   runAsUser: 1000
    #   runAsGroup: 3000
    #   fsGroup: 2000
    podSecurityContext: {}

    # Configure security context for Container
    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    # Example:
    # containerSecurityContext:
    #   enabled: true
    #   runAsUser: 2000
    #   allowPrivilegeEscalation: false
    containerSecurityContext: {}


    ingress:
      # Disabled and done with template instead because this chart assumes nginx :/
      enabled: false

  volume:
    #enabled: true
    #port: 8080
    #grpcPort: 18080
    metricsPort: 9328
    index: leveldb
    replicas: 5
    #minFreeSpacePercent: 5
    #compactionMBps: "250"

    # In the 4.0.0 chart these seemed to change and are now failing... 
    # Though could be that my volumes are actually broken (I know they sorta are) and I need a more permissive health check to fix.
    readinessProbe:
      httpGet:
        path: status
    livenessProbe:
      httpGet:
        path: status

    # For each data disk you may use ANY storage-class, example with local-path-provisioner
    # Annotations are optional.
    #  dataDirs:
    #   - name: data:
    #     type: "persistentVolumeClaim"
    #     size: "24Ti"
    #     storageClass: "local-path-provisioner"
    #     annotations:
    #      "key": "value"
    #     maxVolumes: 0  # If set to zero on non-windows OS, the limit will be auto configured. (default "7")
    #
    # You may also spacify an existing claim:
    #   - name: data
    #     type: "existingClaim"
    #     claimName: "my-pvc"
    #     maxVolumes: 0  # If set to zero on non-windows OS, the limit will be auto configured. (default "7")
    #
    # You can also use emptyDir storage:
    #   - name: data
    #     type: "emptyDir"
    #     maxVolumes: 0  # If set to zero on non-windows OS, the limit will be auto configured. (default "7")
    dataDirs:
      - name: drive
        type: "custom"
        maxVolumes: 0

    extraVolumeMounts: |
      - name: drive
        mountPath: /drive
        subPathExpr: $(POD_NAME)
    extraVolumes: |
      - name: drive
        hostPath: 
          path: /var/mnt/

    # idx can be defined by:
    #
    # idx:
    #  type: "hostPath"
    #  hostPathPrefix: /ssd
    #
    # or
    #
    # idx:
    #  type: "persistentVolumeClaim"
    #  size: "20Gi"
    #  storageClass: "local-path-provisioner"
    #
    # or
    #
    # idx:
    #   type: "existingClaim"
    #   claimName: "myClaim"
    #
    # or
    #
    # idx:
    #  type: "emptyDir"

    # same applies to "logs"

    idx:
      type: "persistentVolumeClaim"
      size: "10Gi"

    logs: 
      type: "persistentVolumeClaim"
      size: "10Gi"

    ingress:
      enabled: false

    # Volume server's rack name
    rack: infra.spencerslab.com

    # Volume server's data center name
    dataCenter: spencerslab.com

    # extraEnvVars is a list of extra enviroment variables to set with the stateful set.
    extraEnvironmentVars:
      PUBLICURL: "$(POD_NAME).infra.spencerslab.com:8080"

    # Redirect moved or non-local volumes. (default proxy)
    #readMode: proxy

    # Affinity Settings
    # Commenting out or setting as empty the affinity variable, will allow
    # deployment to single node services such as Minikube
    affinity: {} #|
      #podAntiAffinity:
      #  requiredDuringSchedulingIgnoredDuringExecution:
      #    - labelSelector:
      #        matchLabels:
      #          app.kubernetes.io/name: {{ template "seaweedfs.name" . }}
      #          app.kubernetes.io/instance: {{ .Release.Name }}
      #          app.kubernetes.io/component: volume
      #      topologyKey: kubernetes.io/hostname

    # Resource requests, limits, etc. for the server cluster placement. This
    # should map directly to the value of the resources field for a PodSpec,
    # formatted as a multi-line string. By default no direct resource request
    # is made.
    resources: {}

    # Configure security context for Pod
    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    # Example:
    podSecurityContext:
      enabled: true
      runAsUser: 2000
      runAsGroup: 2000
      fsGroup: 2000

    # Configure security context for Container
    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    # Example:
    # containerSecurityContext:
    #   enabled: true
    #   runAsUser: 2000
    #   allowPrivilegeEscalation: false
    containerSecurityContext: {}
    
  filer:
    enabled: true
    replicas: 5
    #port: 8888
    #grpcPort: 18888
    metricsPort: 9327
    #encryptVolumeData: false

    # This default is currently overridden globally.
    #  replication type is XYZ:
    # X number of replica in other data centers
    # Y number of replica in other racks in the same data center
    # Z number of replica in other servers in the same rack
    #defaultReplicaPlacement: "000"

    # encrypt data on volume servers
    encryptVolumeData: false

    # Disable http request, only gRpc operations are allowed
    # TODO: This has to be disabled because the health check is hard coded for HTTP.
    disableHttp: false

    # used to configure livenessProbe on filer containers
    livenessProbe:
      enabled: false
      grpc:
        port: 18888

    # used to configure readinessProbe on filer containers
    readinessProbe:
      enabled: false
      grpc:
        port: 18888

    # You may use ANY storage-class, example with local-path-provisioner
    # Annotations are optional.
    #  data:
    #    type: "persistentVolumeClaim"
    #    size: "24Ti"
    #    storageClass: "local-path-provisioner"
    #    annotations:
    #      "key": "value"
    #
    # You may also specify an existing claim:
    #  data:
    #    type: "existingClaim"
    #    claimName: "my-pvc"
    #
    # You can also use emptyDir storage:
    #  data:
    #    type: "emptyDir"
    # or hostpath?: 
      #type: "hostPath"
      #hostPathPrefix: /storage
    data:
      type: "persistentVolumeClaim"
      size: "10Gi"
    logs:
      type: "existingClaim"
      claimName: "data-filer-seaweedfs-filer-0"

    # Affinity Settings
    # Commenting out or setting as empty the affinity variable, will allow
    # deployment to single node services such as Minikube
    affinity: {} #|
      #podAntiAffinity:
      #  requiredDuringSchedulingIgnoredDuringExecution:
      #    - labelSelector:
      #        matchLabels:
      #          app.kubernetes.io/name: {{ template "seaweedfs.name" . }}
      #          app.kubernetes.io/instance: {{ .Release.Name }}
      #          app.kubernetes.io/component: filer
      #      topologyKey: kubernetes.io/hostname

    # Resource requests, limits, etc. for the server cluster placement. This
    # should map directly to the value of the resources field for a PodSpec,
    # formatted as a multi-line string. By default no direct resource request
    # is made.
    resources: {}

    # nodeSelector labels for server pod assignment, formatted as a muli-line string.
    # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
    # Example:
    #nodeSelector: |
    #  kubernetes.io/arch: amd64
    # nodeSelector: |
    #   sw-backend: "true"

    # Configure security context for Pod
    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    # Example:
    # podSecurityContext:
    #   enabled: true
    #   runAsUser: 1000
    #   runAsGroup: 3000
    #   fsGroup: 2000
    podSecurityContext: {}

    # Configure security context for Container
    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    # Example:
    # containerSecurityContext:
    #   enabled: true
    #   runAsUser: 2000
    #   allowPrivilegeEscalation: false
    containerSecurityContext: {}

    ingress:
      enabled: false

    # extraEnvVars is a list of extra enviroment variables to set with the stateful set.
    extraEnvironmentVars:
      WEED_LEVELDB2_ENABLED: "true"
      # with http DELETE, by default the filer would check whether a folder is empty.
      # recursive_delete will delete all sub folders and files, similar to "rm -Rf"
      WEED_FILER_OPTIONS_RECURSIVE_DELETE: "true"
      # directories under this folder will be automatically creating a separate bucket
      WEED_FILER_BUCKETS_FOLDER: "/buckets"

    # secret env variables
    secretExtraEnvironmentVars: {}
        # WEED_POSTGRES_USERNAME:
        #   secretKeyRef:
        #     name: postgres-credentials
        #     key: username
        # WEED_POSTGRES_PASSWORD:
        #   secretKeyRef:
        #     name: postgres-credentials
        #     key: password
  s3:
    enabled: true
    allowEmptyFolder: true
    # Suffix of the host name, {bucket}.{domainName}
    domainName: "s3.spencerslab.com"
    ingress:
      enabled: false
    metricsPort: 9327

    # Resource requests, limits, etc. for the server cluster placement. This
    # should map directly to the value of the resources field for a PodSpec,
    # formatted as a multi-line string. By default no direct resource request
    # is made.
    resources: {}

    # nodeSelector labels for server pod assignment, formatted as a muli-line string.
    # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
    # Example:
    #nodeSelector: |
    #  kubernetes.io/arch: amd64
    # nodeSelector: |
    #   sw-backend: "true"

    # used to assign priority to server pods
    # ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
    #priorityClassName: ""

    # used to assign a service account.
    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
    #serviceAccountName: ""

    # Configure security context for Pod
    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    # Example:
    # podSecurityContext:
    #   enabled: true
    #   runAsUser: 1000
    #   runAsGroup: 3000
    #   fsGroup: 2000
    podSecurityContext: {}

    # Configure security context for Container
    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    # Example:
    # containerSecurityContext:
    #   enabled: true
    #   runAsUser: 2000
    #   allowPrivilegeEscalation: false
    containerSecurityContext: {}

    # You can also use emptyDir storage:
    #  logs:
    #    type: "emptyDir"
    logs:
      type: "existingClaim"
      claimName: "data-s3-seaweedfs-s3-0"

#seaweedfs-volume-server-0:
#  global:
#    monitoring:
#      enabled: true
#    extraEnvironmentVars:
#      WEED_CLUSTER_DEFAULT: "sw"
#      WEED_CLUSTER_SW_MASTER: "seaweedfs-master.default:9333"
#      WEED_CLUSTER_SW_FILER: "seaweedfs-filer-client.default:8888"
#  master:
#    enabled: false
#  filer:  
#    enabled: false
#    s3: 
#      enabled: false
#
#  volume:
#    #enabled: true
#    #port: 8080
#    #grpcPort: 18080
#    metricsPort: 9328
#    index: leveldb
#    replicas: 1
#
#    # In the 4.0.0 chart these seemed to change and are now failing... 
#    # Though could be that my volumes are actually broken (I know they sorta are) and I need a more permissive health check to fix.
#    readinessProbe:
#      httpGet:
#        path: status
#    livenessProbe:
#      httpGet:
#        path: status
#
#    dataDirs:
#      - name: hdd1
#        type: "hostPath"
#        hostPathPrefix: /var/mnt/drive1
#        maxVolumes: 0
#      - name: hdd2
#        type: "hostPath"
#        hostPathPrefix: /var/mnt/drive2
#        maxVolumes: 0
#
#    idx:
#      type: "persistentVolumeClaim"
#      size: "10Gi"
#
#    logs: 
#      type: "persistentVolumeClaim"
#      size: "2Gi"
#
#    ingress:
#      enabled: false
#
#    # Volume server's rack name
#    rack: infra.spencerslab.com
#
#    # Volume server's data center name
#    dataCenter: spencerslab.com
#
#    # extraEnvVars is a list of extra enviroment variables to set with the stateful set.
#    extraEnvironmentVars:
#      WEED_PUBLICURL: "seaweedfs-volume-0.infra.spencerslab.com:8080"
#      WEED_PUBLIC_URL: "seaweedfs-volume-0.infra.spencerslab.com:8080"
#      # TODO: This doesn't work. Have to add the flag to the chart.
#      #WEED_DISK: backup
#
#    # Configure security context for Pod
#    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
#    # Example:
#    podSecurityContext:
#      enabled: true
#      runAsUser: 2000
#      runAsGroup: 2000
#      fsGroup: 2000

samba:
  app-template:
    controllers:
      samba:
        containers:
          main:
            image:
              repository: ghcr.io/crazy-max/samba
              tag: 4.19.9@sha256:270a8dcaeb7be73c3a63e2d7cc966cfdc6ca70b418c59e4af406bee6ea41ded7
            env:
              TZ: America/Denver
              SAMBA_HOSTS_ALLOW: "127.0.0.0/8 10.0.0.0/16"
              SAMBA_SERVER_STRING: &serverstring "storage.spencerslab.com"
              #WSDD2_ENABLE: "1"
              #WSDD2_HOSTNAME: "storage"
              #WSDD2_NETBIOS_NAME: "storage"

    configMaps:
      samba-config:
        data: 
          samba.yaml: |-
            ---
            auth:
              - user: spencer
                group: spencer
                uid: 1000
                gid: 1000
                password_file: /run/secrets/spencer_password
            share:
              - name: media
                path: /mnt/media
                browsable: yes
                writable: yes
                readonly: no
                guestok: no
                validusers: spencer
                writelist: spencer
                veto: no
                hidefiles: /_*/
                recycle: yes
                force user: 1000
              - name: player
                path: /mnt/player
                browsable: yes
                readonly: no
                guestok: no
                public: yes
                validusers: spencer
                writelist: spencer
                veto: no
                hidefiles: /_*/
                recycle: yes
                force user: spencer
              - name: documents
                security: user
                path: /mnt/documents
                browsable: yes
                readonly: no
                guestok: no
                validusers: spencer
                writelist: spencer
                veto: no
                hidefiles: /_*/
                recycle: yes
              - name: pictures
                path: /mnt/pictures
                browsable: yes
                readonly: no
                guestok: no
                validusers: spencer
                writelist: spencer
                veto: no
                hidefiles: /_*/
                recycle: yes
              - name: backups
                create mask: 666
                directory mask: 777
                path: /mnt/backups
                browsable: yes
                readonly: no
                guestok: no
                validusers: spencer
                writelist: spencer
                veto: no
                hidefiles: /_*/
                recycle: yes

    persistence:
      users:
        type: secret
        name: keycloak-secret
        advancedMounts:
          samba:
            main:
              - path: /run/secrets/spencer_password
                subPath: KEYCLOAK_ADMIN_PASSWORD
                readOnly: true
        
      config:
        name: "{{ .Release.Name }}-samba-samba-config"
        type: configMap
        advancedMounts:
          samba:
            main:
              - path: /data/config.yml
                subPath: samba.yaml
                readOnly: true
      media:
        existingClaim: media
        globalMounts:
          - path: /mnt/media
      player:
        existingClaim: player
        globalMounts:
          - path: /mnt/player
      pictures:
        existingClaim: pictures
        globalMounts:
          - path: /mnt/pictures
      documents:
        existingClaim: documents
        globalMounts:
          - path: /mnt/documents
      backups:
        existingClaim: backups
        globalMounts:
          - path: /mnt/backups

forgejo:
  redis-cluster:
    enabled: false
  redis:
    enabled: true
  postgresql:
    enabled: true
  postgresql-ha:
    enabled: false

  persistence:
    enabled: true

  gitea:
    config:
      APP_NAME: "Gitea!!!"
      database:
        DB_TYPE: postgres
      indexer:
        ISSUE_INDEXER_TYPE: bleve
        REPO_INDEXER_ENABLED: true

    admin:
      existingSecret: gitea-admin-secret
      email: gitea@spencerslab.com

    ldap:
      - name: LDAP
        securityProtocol: unencrypted
        host: 'infra-389ds.default'
        port: '389'
        userSearchBase: dc=spencerslab,dc=com
        userFilter: (objectClass=person)
        adminFilter: (CN=brenda)
        emailAttribute: mail
        existingSecret: gitea-secret
        usernameAttribute: CN
        publicSSHKeyAttribute: publicSSHKey

    oauth:
      - name: 'OAuth'
        provider: 'openidConnect'
        existingSecret: gitea-secret
        autoDiscoverUrl: 'https://login.spencerslab.com/realms/SpencersLab/.well-known/openid-configuration'

  deployment:
    containers:
      '0':
        env:
          DISABLE_REGISTRATION: true
